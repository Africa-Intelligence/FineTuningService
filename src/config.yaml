dataset_args:
  dataset_names:
    - africa-intelligence/yahma-alpaca-cleaned-af
    - africa-intelligence/yahma-alpaca-cleaned-zu
    - africa-intelligence/yahma-alpaca-cleaned-xh
    - africa-intelligence/yahma-alpaca-cleaned-tn
  train_split: 0.9
  max_seq_length: 2048

model_args:
  model_name: meta-llama/Meta-Llama-3.1-8B
  lora_rank: 16
  lora_alpha: 16
  lora_dropout: 0
  bias: none

training_args:
  batch_size: 2
  gradient_accumulation_steps: 4
  warmup_steps: 5
  epochs: 2
  max_steps: 60
  learning_rate: 2.0e-4
  logging_steps: 1
  optimizer: adamw_8bit
  weight_decay: 0.01
  lr_scheduler_type: linear
